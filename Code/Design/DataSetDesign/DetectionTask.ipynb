{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../../')\n",
    "from Model import CNN as cn\n",
    "from Model import CommonBlock as cb\n",
    "import numpy as np\n",
    "project_path = os.path.abspath(os.path.relpath('../../../../', os.getcwd()))\n",
    "import Utils.Preprocess as ut\n",
    "data_dir= os.path.join(project_path,'BilinearNetwork\\Data\\PreprocessedData\\CHB-MIT\\detection')\n",
    "constrain_path=os.path.join(project_path,'BilinearNetwork/Data/Constraint/Detection')\n",
    "seizure_table_original=pd.read_csv(os.path.join(constrain_path,'seizure_ictal_summary.csv'))\n",
    "import lightning as L\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,TensorDataset\n",
    "chb_root_path=os.path.join(project_path,'Dataset/CHB-MIT')"
   ],
   "id": "6315bfed03f38532"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b99743a67057ffcf"
  },
  {
   "cell_type": "markdown",
   "id": "4241734a80878bdd",
   "metadata": {},
   "source": [
    "## Detection Latency"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee6908cd1ddda79b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:49:15.475989Z",
     "start_time": "2024-05-22T05:49:15.444503Z"
    }
   },
   "source": [
    "\n",
    "def DetectionLatency(model,file_path,seizure_start,seizure_stop,threshold=0.5,queue_size=10,threshold_count=7):\n",
    "    raw_T=ut.PreprocessTool(file_path).do_preprocess()\n",
    "    def get_latency(_data_stft):\n",
    "        data_ds=TensorDataset(torch.Tensor(_data_stft))\n",
    "        del _data_stft\n",
    "        data_loader=DataLoader(data_ds,batch_size=1)\n",
    "        one_sample_delay=1/raw_T.raw.info['sfreq']\n",
    "        model.eval()\n",
    "        y_pred_queue = deque(maxlen=queue_size)\n",
    "        for i,data in enumerate(tqdm(data_loader)):\n",
    "            data=data[0]\n",
    "            y_pred=model(data).view(-1)\n",
    "            y_pred_queue.append(y_pred.item())\n",
    "            if sum(1 for y in y_pred_queue if y > threshold) >= threshold_count:\n",
    "                current_delay = i * one_sample_delay\n",
    "                model.train()\n",
    "                return current_delay\n",
    "        model.train()\n",
    "        return -1\n",
    "    whole_start,whole_stop=seizure_start-5,seizure_stop+5\n",
    "    partial_start,partial_stop=whole_start,(int(seizure_stop+5)/4)\n",
    "    \n",
    "    data_stft=raw_T.overlap_events_slice_all(start=partial_start,stop=partial_stop).cut_epochs().get_epochs_stft()\n",
    "    latency=get_latency(data_stft)\n",
    "    if(latency==-1):\n",
    "        del data_stft\n",
    "        data_stft=raw_T.overlap_events_slice_all(start=whole_start,stop=whole_stop).cut_epochs().get_epochs_stft()\n",
    "        latency=get_latency(data_stft)\n",
    "    return latency\n",
    "    \n",
    "    \n",
    "    \n",
    "def DetectionLatency_overall(chb_root_path,Net,seizure_table,patient_id,leave_out_id):\n",
    "    patient_name=\"chb\"+str(patient_id).zfill(2)\n",
    "    seizure_table_df = seizure_table[seizure_table['File Name'].str.startswith(patient_name)]\n",
    "    row=seizure_table_df.iloc[leave_out_id,:]\n",
    "    file_name=row['File Name']\n",
    "    file_path=os.path.join(chb_root_path,\"{}/{}\".format(patient_name,file_name))\n",
    "    start_time=row['Start Time']\n",
    "    end_time=row['End Time']\n",
    "    latency=DetectionLatency(model=Net,file_path=file_path,seizure_start=start_time,seizure_stop=end_time)\n",
    "    print(\"Latency:{}\".format(latency))\n",
    "    return latency\n",
    "def analyze_delay(arr):\n",
    "    valid_numbers = [num for num in arr if num != -1]\n",
    "    \n",
    "    mean_value = np.mean(valid_numbers) if valid_numbers else float('nan')\n",
    "    std_value = np.std(valid_numbers) if valid_numbers else float('nan')\n",
    "    \n",
    "    count_negative_ones = arr.count(-1)\n",
    "    count_valid_numbers = len(valid_numbers)\n",
    "    \n",
    "    return {\n",
    "        \"mean\": mean_value,\n",
    "        \"std\": std_value,\n",
    "        \"un_detect\": count_negative_ones,\n",
    "        \"detect\": count_valid_numbers\n",
    "    }\n",
    "def Analyze_write_delay_to_csv(file_path, patient_id, delays):\n",
    "    \"\"\"\n",
    "    Write the metric to the file. if the file does not exist, create the file.\n",
    "    \"\"\"\n",
    "    result=analyze_delay(delays)\n",
    "    delay_mean,delay_std,un_detect_num,detect_num=result['mean'],result['std'],result['un_detect'],result['detect']\n",
    "    delay_str=\"{}({})\".format(delay_mean,delay_std)\n",
    "    detect_num=\"{}/{}\".format(detect_num,detect_num+un_detect_num)\n",
    "    #     check if the file and record exist\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_excel(file_path)\n",
    "        if df[df['patient_id'] == patient_id].shape[0] > 0:\n",
    "            df.loc[df['patient_id'] == patient_id, 'patient_id'] = patient_id\n",
    "            df.loc[df['patient_id'] == patient_id, 'delay'] = delay_str\n",
    "            df.loc[df['patient_id'] == patient_id, 'detected'] = detect_num\n",
    "        else:\n",
    "            df = pd.concat(\n",
    "                [df, pd.DataFrame({'patient_id': [patient_id], 'delay': [delay_str], 'detected': [detect_num]})],\n",
    "                ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame({'patient_id': [patient_id], 'delay': [delay_str], 'detected': [detect_num]})\n",
    "    df.to_excel(file_path, index=False)\n",
    "    "
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "61d784e6a9516788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:49:15.492101Z",
     "start_time": "2024-05-22T05:49:15.475989Z"
    }
   },
   "source": [
    "# Net = cb.CommonNet(encoder=cn.ConvNetBlock_small(), lr=0.0003)\n",
    "# DetectionLatency_overall(chb_root_path=chb_root_path,seizure_table=seizure_table_original,Net=Net,patient_id=1,leave_out_id=0)\n",
    "# --\n",
    "# Analyze_write_delay_to_csv(r'E:\\Research\\BilinearNetwork\\Data\\Result\\detection\\detect_seizure\\1.xlsx',1,[1,1,-1,-1,-1,6,1.3,5])"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3b8afee26d497dfa",
   "metadata": {},
   "source": [
    "## FDR"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4aa78698dede590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:49:15.523667Z",
     "start_time": "2024-05-22T05:49:15.492101Z"
    }
   },
   "source": [
    "def get_interictal_files_name(chb_root_path,constrain_path,patient_id):\n",
    "    def find_all_files(chb_root_path):\n",
    "        all_files=[]\n",
    "        for root,dirs,files in os.walk(chb_root_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.edf'):\n",
    "                    all_files.append(file)\n",
    "        return all_files\n",
    "    exclude_file=pd.read_csv(os.path.join(constrain_path,'exclude_File.csv'))\n",
    "    exclude_patient=pd.read_csv(os.path.join(constrain_path,'exclude_Patient.csv'))\n",
    "    small_constrant = list(exclude_file['File Name'])\n",
    "    exclude_patient = list(exclude_patient['0'])\n",
    "    large_table = find_all_files(chb_root_path)\n",
    "    select_files = large_table.copy()\n",
    "    for file_name in large_table:\n",
    "        prelix = file_name.split('_')[0]\n",
    "        if prelix in exclude_patient:\n",
    "            select_files.remove(file_name)\n",
    "            continue\n",
    "        if file_name in small_constrant:\n",
    "            select_files.remove(file_name)\n",
    "            continue\n",
    "    select_files=pd.Series(select_files)\n",
    "    select_file_final = []\n",
    "    for i in range(len(select_files) - 1):\n",
    "        current_num = int(select_files[i].split('_')[1].split('.')[0])\n",
    "        prelix = select_files[i].split('_')[0]\n",
    "        next_num = int(select_files[i + 1].split('_')[1].split('.')[0])\n",
    "        if abs(next_num - current_num) > 1:\n",
    "            select_file_final.append(select_files[i])\n",
    "            previous_file_name = prelix + '_' + str(current_num - 1).zfill(2) + '.edf'\n",
    "            previous_previous_file_name = prelix + '_' + str(current_num - 2).zfill(2) + '.edf'\n",
    "            if previous_file_name in select_files:\n",
    "                select_file_final.append(previous_file_name)\n",
    "            if previous_previous_file_name in select_files:\n",
    "                select_file_final.append(previous_previous_file_name)\n",
    "    select_file_final.extend(\n",
    "        ['chb19_07.edf', 'chb19_10.edf', 'chb19_15.edf'])\n",
    "    select_file_final.sort()\n",
    "    files=pd.Series(select_file_final,name='file')\n",
    "    patient_name=\"chb\"+str(patient_id).zfill(2)\n",
    "    files=files[files.str.startswith(patient_name)].drop_duplicates()\n",
    "    \n",
    "    s_files=select_files[select_files.str.startswith(patient_name)].drop_duplicates()\n",
    "    \n",
    "    return files,s_files[~s_files.isin(files)]\n",
    "\n",
    "def FDR_for_one_ictal_file(model,file_path,threshold=0.5,threshold_count=7):\n",
    "    raw_T=ut.PreprocessTool(file_path).do_preprocess(truncate_time=3600)\n",
    "    duration=raw_T.raw.n_times / raw_T.raw.info['sfreq']\n",
    "    _data_stft=raw_T.create_group_slicing_event(group_interval=20,num_events_per_group=10,duration=5).cut_epochs().get_epochs_stft()\n",
    "    data_ds=TensorDataset(torch.Tensor(_data_stft))\n",
    "    del _data_stft\n",
    "    data_loader=DataLoader(data_ds,batch_size=10)\n",
    "    alarm_count=0\n",
    "    model.eval()\n",
    "    for i,data in enumerate(data_loader):\n",
    "        data=data[0]\n",
    "        y_pred=model(data).view(-1)\n",
    "        if sum(1 for y in y_pred if y > threshold) >= threshold_count:\n",
    "            alarm_count+=1\n",
    "    model.train()\n",
    "    FDR_per_second=alarm_count/duration\n",
    "    return FDR_per_second\n",
    "    \n",
    "\n",
    "\n",
    "def FDR_for_patient_ictal_file(chb_root_path,constrain_path,model,patient_id,**kwargs):\n",
    "    print(\"Start evaluate the FDR for algorithm\")\n",
    "    files_plan=get_interictal_files_name(chb_root_path=chb_root_path,constrain_path=constrain_path,patient_id=patient_id)\n",
    "    patient_name=\"chb\"+str(patient_id).zfill(2)\n",
    "    FDR_per_seconds=[]\n",
    "    for ictal_file_name in files_plan:\n",
    "        file_path=os.path.join(chb_root_path,\"{}/{}\".format(patient_name,ictal_file_name))\n",
    "        one_FDR=FDR_for_one_ictal_file(model,file_path,kwargs)\n",
    "        FDR_per_seconds.append(one_FDR)\n",
    "    return np.mean(FDR_per_seconds)*3600\n",
    "# FDR_for_patient_ictal_file(chb_root_path=chb_root_path,constrain_path=constrain_path,model=cb.CommonNet(cn.ConvNetBlock_small()),patient_id=1)\n",
    "\n",
    "\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "b8322a4b116daee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T05:49:32.529366Z",
     "start_time": "2024-05-22T05:49:32.488831Z"
    }
   },
   "source": [
    "get_interictal_files_name(chb_root_path,constrain_path,18)\n",
    "\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91458d3-678d-4ac6-b10b-6437c7f65ba8",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
